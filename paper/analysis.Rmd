




```{r read_data}
library(tidyverse)

data <- read_rds("../data/alldata.rds")
```

We use four examples (see Figure 1) to illustrate how the agents change their opinion during the simulation and how different the opinions look at the end of the simulation. As can be seen at the top left of Figure 1, one possible outcome is that the opinions of the agents diverge completely and only two extreme opinions are formed. After less than 15 simulation steps, every agent has either opinion 0.00 or opinion 1.00. In this example, the epsilon is low and the backfire effect takes place. 

```{r, out.width="1\\linewidth", include=TRUE, fig.align="center", fig.cap=c("your caption"), echo=FALSE}
knitr::include_graphics("../examples.pdf")
```

In comparison to this example, in the third example (bottom left) no backfire effect takes place. In both examples, the epsilon is 0.1. Comparing the two examples, it becomes clear that the backfire effect increases the divergence of opinions. While in the first example two clear opinions quickly establish, in the third example there are more different opinions for a longer time. After 20 simulation steps, two groups of agents form whose opinions are similar to each other. Nevertheless, even after 30 simulation steps, these agents still have similar opinions but not one uniform opinion. 

In example 4 (bottom right), also no backfire takes place. Here the different opinions converge to a consensus of opinion. After around 20 simulation steps each agent has the opinion 0.5. 

In contrast, in example 2 (top right) no majority opinion develops, but several groups with the same opinions form. In this example, the epsilon is higher than in the other examples, which leads the agents to accept opinions that differ more from their own than in the other examples. 
 
## Influence of programming language, epsilon and backfire on opinion count
After we looked at the opinion formation of the agents itself, we now consider, whether the epsilon, if the backfire effect takes place or not and the programming language has an influence on the existence of different opinions. To look at the influence of the enumerated factors, we consider (see Figure 2) how many different opinions exist (y-axis). We further consider the standard deviations of the opinions (color) to analyse how different the opinions are.

```{r all_runs}
data %>% group_by(run_number, epsilon, backfire, agent_count, program) %>% 
  summarise(opinion_mean = mean(list_op),
            opinion_sd = sd(list_op),
            opinion_count = length(levels(factor(list_op)))) %>% 
  ungroup() %>% 
  mutate(backfire = factor(backfire, labels = c("no backfire", "backfire"))) %>% 
  ggplot() +
  aes(x = factor(epsilon)) +
  aes(color = opinion_sd) +
  aes(y = opinion_count) +
  #aes(color = factor(program)) +
  geom_jitter(alpha = 0.5) +
  facet_grid(backfire~program, scales = "free_y") +
  theme_bw()

```

As Figure 2 shows, if the epsilon is higher than 0.55, practically all agents have only one opinion (sd = 0.0), regardless of whether the backfire effect takes place or not and which programming language is used. When the epsilon is lower than 0.55 and the backfire effect takes place, there are two opinions among the agents that diverge to the two extremes of opinion (sd = 0.5). In comparison, when the epsilon is lower than 0.55 and no backfire effect takes place, the agents have more different opinions, but the standard deviations of the opinions are lower (less bright) than in the simulations with backfire effect. The lower the epsilon is, the higher is the amount of opinions. Comparing the two programming languages, the amount of different opinions is a bit higher when NetLogo is used, but the difference is small. Overall, the two programming languages showed almost the same qualitative results. As Table 1 shows, the quantitative comarison of both languages showed, that except of three simulation runs, the t.test wasnÂ´t significant. Thus the languages showed the same results. 

```{r table}
library(broom)


data %>% group_by(run_number, epsilon, backfire, agent_count, program) %>% 
  summarise(opinion_mean = mean(list_op),
            opinion_sd = sd(list_op),
            opinion_count = length(levels(factor(list_op)))) %>%
  group_by(epsilon, backfire, agent_count) %>% 
  do(tidy(t.test(opinion_sd ~ program, data = .))) %>% 
  mutate(conf_width = abs(conf.low - conf.high )) %>% 
  arrange(p.value) %>% head(10) %>% 
  select(epsilon, backfire, agent_count, t = statistic, p = p.value, df = parameter ) %>% 
  knitr::kable(caption = "Overview of strongest differences between languages", booktabs = T, longtable = F)
  
```

